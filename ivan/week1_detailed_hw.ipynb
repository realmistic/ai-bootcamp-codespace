{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638b0ad7",
   "metadata": {},
   "source": [
    "# 0) Prep functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import io\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import requests\n",
    "import frontmatter\n",
    "from minsearch import Index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 repo_owner: str,\n",
    "                 repo_name: str,\n",
    "                 allowed_extensions: set[str] | None = None,\n",
    "                 path_filter: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "\n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "            path_filter: Optional path prefix to filter files (e.g., \"_podcast\")\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "\n",
    "        self.allowed_extensions = allowed_extensions\n",
    "        self.path_filter = path_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "\n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "\n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "\n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        # Skip directories\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # Skip hidden files\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        # Filter by path prefix if specified\n",
    "        if self.path_filter and not filepath.startswith(self.path_filter):\n",
    "            return True\n",
    "\n",
    "        # Filter by extension if specified\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "\n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "\n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "\n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "\n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ab9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_podcast_data():\n",
    "    \"\"\"\n",
    "    Question 4: Download podcast data from DataTalks.Club GitHub repository.\n",
    "\n",
    "    Returns:\n",
    "        List of RawRepositoryFile objects containing podcast data\n",
    "    \"\"\"\n",
    "    repo_owner = 'DataTalksClub'\n",
    "    repo_name = 'datatalksclub.github.io'\n",
    "\n",
    "    # Only get files from _podcast directory\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        path_filter=\"_podcast\"\n",
    "    )\n",
    "\n",
    "    data = reader.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a4df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_podcast_data(data_raw):\n",
    "    \"\"\"\n",
    "    Parse markdown files with frontmatter to extract metadata and content.\n",
    "\n",
    "    Args:\n",
    "        data_raw: List of RawRepositoryFile objects\n",
    "\n",
    "    Returns:\n",
    "        List of parsed dictionaries with filename, metadata, and content\n",
    "    \"\"\"\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        try:\n",
    "            post = frontmatter.loads(f.content)\n",
    "            data = post.to_dict()\n",
    "            data['filename'] = f.filename\n",
    "            data_parsed.append(data)\n",
    "        except Exception as e:\n",
    "            # Skip files with parsing errors (like templates)\n",
    "            print(f\"Skipping {f.filename}: {type(e).__name__}\")\n",
    "            continue\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a8bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(seq: List[Any], size: int, step: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (list of paragraphs) to be chunked\n",
    "        size: The size of each chunk/window (number of paragraphs)\n",
    "        step: The step size between consecutive windows\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk\n",
    "            - 'content': The chunk content (joined paragraphs)\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        # Join paragraphs back with double newlines\n",
    "        content = '\\n\\n'.join(batch)\n",
    "        result.append({'start': i, 'content': content})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4deeb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_documents_by_paragraphs(\n",
    "    documents: List[Dict[str, Any]],\n",
    "    size: int = 30,\n",
    "    step: int = 15,\n",
    "    content_field_name: str = 'content'\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks by paragraphs using sliding windows.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries with content field\n",
    "        size: Number of paragraphs per chunk (default: 30)\n",
    "        step: Step size between chunks (default: 15, creates overlap of size-step)\n",
    "        content_field_name: Name of the field containing document content\n",
    "\n",
    "    Returns:\n",
    "        List of chunk dictionaries with metadata preserved\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name, '')\n",
    "\n",
    "        # Skip if no content\n",
    "        if not doc_content:\n",
    "            continue\n",
    "\n",
    "        # Split content by paragraphs (double newlines)\n",
    "        paragraphs = doc_content.split('\\n\\n')\n",
    "        # Filter out empty paragraphs\n",
    "        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "        # Create chunks using sliding window\n",
    "        chunks = sliding_window(paragraphs, size=size, step=step)\n",
    "\n",
    "        # Add document metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c297691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_podcasts(chunks, query, num_results=5):\n",
    "    \"\"\"\n",
    "    Search podcast chunks using minsearch.\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        query: Search query string\n",
    "        num_results: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    # Create index with content as the main text field\n",
    "    index = Index(text_fields=[\"content\"])\n",
    "\n",
    "    # Fit the index with chunks\n",
    "    index.fit(chunks)\n",
    "\n",
    "    # Search for the query\n",
    "    results = index.search(query=query, num_results=num_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d73df4",
   "metadata": {},
   "source": [
    "# 1) Q4 -- downloads podcast and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00cd5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Question 4: Downloading podcast data from DataTalks.Club...\n",
      "======================================================================\n",
      "\n",
      "Number of podcast records: 185\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Question 4: Downloading podcast data from DataTalks.Club...\")\n",
    "print(\"=\" * 70)\n",
    "podcast_files = download_podcast_data()\n",
    "\n",
    "print(f\"\\nNumber of podcast records: {len(podcast_files)}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa83b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example filenames:\n",
      "  1. _podcast/_s12e08.md\n",
      "  2. _podcast/_template.md\n",
      "  3. _podcast/s01e01-roles.md\n",
      "  4. _podcast/s01e02-processes.md\n",
      "  5. _podcast/s01e03-building-ds-team.md\n"
     ]
    }
   ],
   "source": [
    "# Show a few example filenames\n",
    "print(\"\\nExample filenames:\")\n",
    "for i, file in enumerate(podcast_files[:5]):\n",
    "    print(f\"  {i+1}. {file.filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0b4c5",
   "metadata": {},
   "source": [
    "# 2) Q5 Chunking podcast data by paragraphs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2654596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 5: Chunking podcast data by paragraphs...\n",
      "======================================================================\n",
      "Skipping _podcast/_template.md: ConstructorError\n",
      "\n",
      "Parsed 184 podcast documents\n",
      "\n",
      "Number of chunks created: 162\n",
      "\n",
      "Example chunk:\n",
      "  Filename: _podcast/_s12e08.md\n",
      "  Start position: 0\n",
      "  Content preview: Links:\n",
      "\n",
      "* [Jekaterina's LinkedIn](https://www.linkedin.com/in/jekaterina-kokatjuhha/){:target=\"_blank\"}...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Question 5: Chunking podcast data by paragraphs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Parse the markdown files\n",
    "parsed_data = parse_podcast_data(podcast_files)\n",
    "print(f\"\\nParsed {len(parsed_data)} podcast documents\")\n",
    "\n",
    "# Chunk by paragraphs with size=30, overlap=15\n",
    "chunks = chunk_documents_by_paragraphs(\n",
    "    parsed_data,\n",
    "    size=30,\n",
    "    step=15\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of chunks created: {len(chunks)}\")\n",
    "\n",
    "# Show example chunk info\n",
    "if chunks:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Filename: {chunks[0].get('filename', 'N/A')}\")\n",
    "    print(f\"  Start position: {chunks[0]['start']}\")\n",
    "    print(f\"  Content preview: {chunks[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6c305",
   "metadata": {},
   "source": [
    "# 3) Q6 Searching with minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd6a26f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 6: Searching with minsearch...\n",
      "======================================================================\n",
      "\n",
      "Query: 'how do I make money with AI?'\n",
      "\n",
      "Found 5 results\n",
      "\n",
      "First episode in results: _podcast/s13e06-secret-sauce-of-data-science-management.md\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. Episode: _podcast/s13e06-secret-sauce-of-data-science-management.md\n",
      "   Preview: Links:\n",
      "\n",
      "* [The secret sauce of data science management](https://www.youtube.com/watch?v=tbBfVHIh-38){:target=\"_blank\"}\n",
      "* [Lessons learned leading AI t...\n",
      "\n",
      "2. Episode: _podcast/s03e01-from-pm-to-ds.md\n",
      "   Preview: We talked about:\n",
      "\n",
      "- Kseniaâ€™s background\n",
      "- Data analytics vs data science\n",
      "- Skills needed for data analytics and data science\n",
      "- Benefits of getting a m...\n",
      "\n",
      "3. Episode: _podcast/s10e03-mlops-architect.md\n",
      "   Preview: Links:\n",
      "\n",
      "* [Matt Turck](https://mattturck.com/data2021/){:target=\"_blank\"}\n",
      "* [AI Observability Platform](https://whylabs.ai/observability){:target=\"_bl...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Question 6: Searching with minsearch...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Search for the query\n",
    "query = \"how do I make money with AI?\"\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "\n",
    "results = search_podcasts(chunks, query, num_results=5)\n",
    "\n",
    "print(f\"\\nFound {len(results)} results\")\n",
    "\n",
    "if results:\n",
    "    first_result = results[0]\n",
    "    first_episode = first_result.get('filename', 'N/A')\n",
    "\n",
    "    print(f\"\\nFirst episode in results: {first_episode}\")\n",
    "    print(f\"\\nTop 3 results:\")\n",
    "    for i, result in enumerate(results[:3], 1):\n",
    "        episode = result.get('filename', 'N/A')\n",
    "        content_preview = result.get('content', '')[:150]\n",
    "        print(f\"\\n{i}. Episode: {episode}\")\n",
    "        print(f\"   Preview: {content_preview}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcf2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
